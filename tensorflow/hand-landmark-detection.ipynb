{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pose Estimation Overview\n",
    "\n",
    "## 1. What is Pose Estimation?\n",
    "Pose estimation is a computer vision technique used to identify and track human body parts and their positions in an image or video. It predicts the coordinates of key points (e.g., joints, limbs) to model the pose or movement of the body.\n",
    "\n",
    "## 2. Software for Pose Estimation:\n",
    "Common software and frameworks include:\n",
    "- **MediaPipe:** A lightweight framework by Google for real-time pose detection.\n",
    "- **OpenPose:** An open-source library for multi-person pose estimation.\n",
    "- **TensorFlow.js or PyTorch:** For implementing custom pose estimation models like PoseNet.\n",
    "- **BlazePose:** Built specifically for fast and accurate single-person pose estimation.\n",
    "\n",
    "## 3. What is MediaPipe?\n",
    "MediaPipe is a cross-platform framework by Google that offers efficient pipelines for machine learning and computer vision tasks, such as pose estimation, hand tracking, and facial landmark detection. It supports mobile devices, web, and desktop environments with pre-trained models.\n",
    "\n",
    "\n",
    "## 4. Landmarks in MediaPipe:\n",
    "Landmarks are specific points on the body that MediaPipe detects to represent key parts like joints (e.g., elbows, knees) or regions (e.g., shoulders, hips).\n",
    "- For pose estimation, MediaPipe's **Pose** solution identifies 33 3D landmarks across the human body, enabling applications like fitness tracking, gesture recognition, and AR/VR interactions.\n",
    "![image](https://ai.google.dev/static/edge/mediapipe/images/solutions/hand-landmarks.png)\n",
    "## 5. Brief Explanation:\n",
    "Pose estimation using MediaPipe is highly efficient and lightweight, making it suitable for real-time applications on mobile and desktop platforms. Its built-in models for detecting landmarks are pre-trained, ensuring high accuracy with minimal resource usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 10:40:17.405179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 10:40:18.265561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes modules from the Mediapipe library, which is used for various machine learning solutions, including hand tracking and gesture recognition. Here's what each line means:\n",
    "```python\n",
    "mp_hands = mp.solutions.hands\n",
    "```\n",
    "`mp.solutions.hands` refers to the Hands solution provided by Mediapipe.\n",
    "This module is designed for hand detection and tracking, including landmark estimation for each finger joint.\n",
    "By assigning it to mp_hands, you create a shorthand to access its functionality in your code.\n",
    "```python\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "```\n",
    "`mp.solutions.drawing_utils` is a utility module for visualizing results.\n",
    "It includes functions to draw detected landmarks and connections (like joints and bones in the hand) on images or video frames.\n",
    "By assigning it to mp_drawing, you can easily use these drawing functions to display the detected hand landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Feature Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(frame, hands):\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process frame with Mediapipe\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Frame dimensions\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Initialize numpy arrays for left and right hands\n",
    "    left_hand_landmarks = np.zeros((21, 2))\n",
    "    right_hand_landmarks = np.zeros((21, 2))\n",
    "\n",
    "    # Helper function to process landmarks\n",
    "    def process_landmarks(hand_landmarks, width, height):\n",
    "        landmarks = [(lm.x * width, lm.y * height) for lm in hand_landmarks.landmark]\n",
    "        landmark_0 = np.array(landmarks[0])\n",
    "        landmark_5 = np.array(landmarks[5])\n",
    "        normalized_landmarks = [\n",
    "            ((x - landmark_0[0]) / (landmark_5[0] - landmark_0[0] + 1e-6),\n",
    "             (y - landmark_0[1]) / (landmark_5[1] - landmark_0[1] + 1e-6))\n",
    "            for x, y in landmarks\n",
    "        ]\n",
    "        return np.array(normalized_landmarks)\n",
    "\n",
    "    # If hands are detected\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            # Identify hand as left or right\n",
    "            handedness = hand_handedness.classification[0].label\n",
    "            processed_landmarks = process_landmarks(hand_landmarks, width, height)\n",
    "            if handedness == 'Left':\n",
    "                left_hand_landmarks = processed_landmarks\n",
    "            elif handedness == 'Right':\n",
    "                right_hand_landmarks = processed_landmarks\n",
    "\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Concatenate left and right hand landmarks\n",
    "    concatenated_landmarks = np.concatenate((left_hand_landmarks.flatten(), right_hand_landmarks.flatten()))\n",
    "    return concatenated_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code explanation `feature_extract`\n",
    "\n",
    "The function `feature_extract` extracts features from a video frame by processing hand landmarks detected by Mediapipe Hands. It outputs normalized 2D landmark coordinates for both left and right hands.\n",
    "\n",
    "### Parameters\n",
    "1. `frame`: A single video frame (image) from which hand landmarks are to be extracted.\n",
    "1. `hands`: An instance of Mediapipe's `Hands` class for hand detection and landmark tracking.\n",
    "### Function outputs\n",
    "Output\n",
    "\n",
    "The function returns a 1D numpy array containing 84 values:\n",
    "\n",
    "1. 42 values for the left hand (21 points × 2 coordinates: x, y).\n",
    "2. 42 values for the right hand (21 points × 2 coordinates: x, y).\n",
    "\n",
    "If no hand is detected, the corresponding values are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks Reader and Dataset Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_landmarks(dataset, labels):\n",
    "    # Initialize Mediapipe Hands and Drawing\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=2,\n",
    "                           min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "    kelas = np.eye(len(labels))\n",
    "    y = []\n",
    "    \"\"\"\n",
    "    Reads all `.jpg` images from multiple label directories inside the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The base directory of the dataset.\n",
    "        labels (list): List of label names (subdirectories).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are label names and values are lists of image frames.\n",
    "    \"\"\"\n",
    "    images_by_label = {}\n",
    "    X = []\n",
    "    for i, label in enumerate(labels):\n",
    "        # Construct the directory path\n",
    "        directory_path = os.path.join(dataset, label)\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"Directory '{directory_path}' does not exist.\")\n",
    "            images_by_label[label] = []\n",
    "            continue\n",
    "\n",
    "        # List all `.jpg` files in the directory\n",
    "        filenames = [f for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "\n",
    "        # Read images and store them as frames\n",
    "        # frames = []\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            frame = cv2.imread(file_path)  # Read the image with OpenCV\n",
    "            fitur = feature_extract(frame, hands)\n",
    "            X.append(fitur)\n",
    "            y.append(kelas[i])\n",
    "\n",
    "    hands.close()\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: `read_landmarks`\n",
    "\n",
    "The function `read_landmarks` is designed to:\n",
    "\n",
    "1. Traverse a dataset organized by labels (subdirectories).\n",
    "1. Read all `.jpg` images within each subdirectory.\n",
    "1. Extract features from each image using Mediapipe.\n",
    "1. Prepare the data (`X`) and labels (`y`) for use in machine learning models.\n",
    "\n",
    "### Function Breakdown\n",
    "Parameters\n",
    "\n",
    "1. `dataset` (str): The root directory containing subdirectories for each label.\n",
    "1. `labels` (list): A list of label names, where each corresponds to a subdirectory in the `dataset`.\n",
    "\n",
    "### Step 1: Mediapipe Hands is initialized to process images:\n",
    "\n",
    "1. `static_image_mode=False`: Operates in dynamic mode for video or multiple frames.\n",
    "1. `max_num_hands=2`: Tracks up to 2 hands.\n",
    "1. `min_detection_confidence=0.5`: Minimum confidence to detect a hand.\n",
    "1. `min_tracking_confidence=0.5`: Minimum confidence for hand landmark tracking.\n",
    "\n",
    "### Step 2: One-Hot Encode Labels\n",
    "```python\n",
    "kelas = np.eye(len(labels))\n",
    "y = []\n",
    "```\n",
    "A one-hot encoded matrix (`kelas`) is created for the labels. For example, if there are 3 labels:\n",
    "```lua\n",
    "[[1, 0, 0],\n",
    " [0, 1, 0],\n",
    " [0, 0, 1]]\n",
    "```\n",
    "`y` is initialized to store the labels corresponding to each image.\n",
    "\n",
    "### Step 3: Iterate Over Labels\n",
    "```python\n",
    "for i, label in enumerate(labels):\n",
    "    directory_path = os.path.join(dataset, label)\n",
    "```\n",
    "Loops through each label and constructs the full path to its corresponding directory.\n",
    "\n",
    "### Step 4: List All `.jpg` Files\n",
    "```python\n",
    "filenames = [f for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "```\n",
    "Retrieves all filenames ending in `.jpg` from the directory.\n",
    "\n",
    "### Step 5: Read and Process Images\n",
    "```python\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    frame = cv2.imread(file_path)\n",
    "    fitur = ekstraksi_fitur(frame, hands)\n",
    "    X.append(fitur)\n",
    "    y.append(kelas[i])\n",
    "```\n",
    "\n",
    "1. For each image file:\n",
    "    1. Constructs the full file path.\n",
    "    2. Reads the image using OpenCV (`cv2.imread`).\n",
    "    3. Calls `feature_extract` to extract hand landmark features\n",
    "    4. Normalized features for left and right hands are extracted using Mediapipe.\n",
    "    5. Appends the extracted features to `X` and the corresponding one-hot encoded label to `y`.\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "1. `X`: A numpy array of extracted features for all images.\n",
    "1. `y`: A numpy array of one-hot encoded labels corresponding to each feature in `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion from numpy array to Tensor\n",
    "The following function is used to convert from numpy array to Tensor. This conversion is intended for Pytorch dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Landmark Extraction using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "dataset = \"../dataset\"                  # Base dataset directory\n",
    "labels = [\"Satu\", \"Dua\"]     # List of labels (subdirectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732160524.693378   40724 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1732160524.696136   41948 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: llvmpipe (LLVM 15.0.7, 256 bits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 84) (49, 2) (13, 84) (13, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function\n",
    "X,y = read_landmarks(dataset, labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "input_size = X_train.shape[1]  # Number of features (flattened landmarks)\n",
    "num_classes = len(labels)      # Number of labels\n",
    "batch_size = 32                # Batch size for DataLoader\n",
    "learning_rate = 0.001          # Learning rate\n",
    "num_epochs = 50                # Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 84, 1)             0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 82, 32)            128       \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 80, 64)            6208      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 5120)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               655488    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 662082 (2.53 MB)\n",
      "Trainable params: 662082 (2.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create model, loss function, and optimizer\n",
    "model = get_model(input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=SGD(learning_rate=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 10:41:24.932155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2024-11-21 10:41:25.553161: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x70baf3f33ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-21 10:41:25.553204: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2024-11-21 10:41:25.562329: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2024-11-21 10:41:25.604294: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 355ms/step - loss: 0.7274 - accuracy: 0.4615 - val_loss: 0.5930 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6958 - accuracy: 0.5641 - val_loss: 0.5617 - val_accuracy: 0.7000\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6968 - accuracy: 0.5385 - val_loss: 0.5524 - val_accuracy: 0.7000\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6206 - accuracy: 0.6410 - val_loss: 0.5499 - val_accuracy: 0.8000\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6214 - accuracy: 0.6923 - val_loss: 0.5481 - val_accuracy: 0.8000\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6318 - accuracy: 0.6667 - val_loss: 0.5453 - val_accuracy: 0.8000\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6027 - accuracy: 0.6154 - val_loss: 0.5435 - val_accuracy: 0.9000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5902 - accuracy: 0.7692 - val_loss: 0.5432 - val_accuracy: 0.9000\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6243 - accuracy: 0.6154 - val_loss: 0.5417 - val_accuracy: 0.9000\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6547 - accuracy: 0.5641 - val_loss: 0.5387 - val_accuracy: 0.9000\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6415 - accuracy: 0.6667 - val_loss: 0.5374 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5894 - accuracy: 0.6667 - val_loss: 0.5362 - val_accuracy: 0.9000\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6526 - accuracy: 0.5897 - val_loss: 0.5349 - val_accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6289 - accuracy: 0.6154 - val_loss: 0.5345 - val_accuracy: 0.9000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6257 - accuracy: 0.6667 - val_loss: 0.5342 - val_accuracy: 0.9000\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5997 - accuracy: 0.6667 - val_loss: 0.5337 - val_accuracy: 0.9000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6023 - accuracy: 0.6410 - val_loss: 0.5332 - val_accuracy: 0.9000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5844 - accuracy: 0.7436 - val_loss: 0.5326 - val_accuracy: 0.9000\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5992 - accuracy: 0.6667 - val_loss: 0.5320 - val_accuracy: 0.9000\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6059 - accuracy: 0.7692 - val_loss: 0.5308 - val_accuracy: 0.9000\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6025 - accuracy: 0.7692 - val_loss: 0.5305 - val_accuracy: 0.9000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5694 - accuracy: 0.7436 - val_loss: 0.5284 - val_accuracy: 0.9000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6114 - accuracy: 0.6154 - val_loss: 0.5276 - val_accuracy: 0.9000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6072 - accuracy: 0.5897 - val_loss: 0.5270 - val_accuracy: 0.9000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5726 - accuracy: 0.7179 - val_loss: 0.5265 - val_accuracy: 0.9000\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6067 - accuracy: 0.5897 - val_loss: 0.5262 - val_accuracy: 0.9000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6127 - accuracy: 0.6667 - val_loss: 0.5256 - val_accuracy: 0.9000\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5911 - accuracy: 0.6667 - val_loss: 0.5247 - val_accuracy: 0.9000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5883 - accuracy: 0.6923 - val_loss: 0.5242 - val_accuracy: 0.9000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5821 - accuracy: 0.6667 - val_loss: 0.5238 - val_accuracy: 0.9000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.5905 - accuracy: 0.6667 - val_loss: 0.5231 - val_accuracy: 0.9000\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6080 - accuracy: 0.6154 - val_loss: 0.5223 - val_accuracy: 0.9000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6145 - accuracy: 0.6667 - val_loss: 0.5225 - val_accuracy: 0.9000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5870 - accuracy: 0.6410 - val_loss: 0.5220 - val_accuracy: 0.9000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5744 - accuracy: 0.8205 - val_loss: 0.5217 - val_accuracy: 0.9000\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.5797 - accuracy: 0.7436 - val_loss: 0.5211 - val_accuracy: 0.9000\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.5711 - accuracy: 0.6154 - val_loss: 0.5205 - val_accuracy: 0.9000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5827 - accuracy: 0.7436 - val_loss: 0.5209 - val_accuracy: 0.8000\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.5906 - accuracy: 0.6410 - val_loss: 0.5205 - val_accuracy: 0.9000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.5636 - accuracy: 0.6923 - val_loss: 0.5194 - val_accuracy: 0.9000\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6215 - accuracy: 0.6410 - val_loss: 0.5190 - val_accuracy: 0.9000\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6006 - accuracy: 0.7179 - val_loss: 0.5183 - val_accuracy: 0.9000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6069 - accuracy: 0.7179 - val_loss: 0.5175 - val_accuracy: 0.9000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5683 - accuracy: 0.6923 - val_loss: 0.5170 - val_accuracy: 0.9000\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5949 - accuracy: 0.6667 - val_loss: 0.5165 - val_accuracy: 0.9000\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5514 - accuracy: 0.7179 - val_loss: 0.5159 - val_accuracy: 0.8000\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.5622 - accuracy: 0.6667 - val_loss: 0.5148 - val_accuracy: 0.9000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5452 - accuracy: 0.6923 - val_loss: 0.5146 - val_accuracy: 0.8000\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5758 - accuracy: 0.6923 - val_loss: 0.5147 - val_accuracy: 0.8000\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.5805 - accuracy: 0.6667 - val_loss: 0.5146 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.54 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving for further purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"outputs\"\n",
    "model_name = \"model.keras\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)  # Buat direktori jika belum ada\n",
    "    print(f\"Directory '{output_path}' created.\")\n",
    "model.save(os.path.join(output_path,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "\n",
    "Let's load our model from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "model_path = \n",
    "model = tf.keras.models.load_model(\"saved_model/my_model\")\n",
    "\n",
    "# Make predictions (optional)\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "# print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
