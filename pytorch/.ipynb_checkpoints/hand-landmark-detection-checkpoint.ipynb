{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pose Estimation Overview\n",
    "\n",
    "## 1. What is Pose Estimation?\n",
    "Pose estimation is a computer vision technique used to identify and track human body parts and their positions in an image or video. It predicts the coordinates of key points (e.g., joints, limbs) to model the pose or movement of the body.\n",
    "\n",
    "## 2. Software for Pose Estimation:\n",
    "Common software and frameworks include:\n",
    "- **MediaPipe:** A lightweight framework by Google for real-time pose detection.\n",
    "- **OpenPose:** An open-source library for multi-person pose estimation.\n",
    "- **TensorFlow.js or PyTorch:** For implementing custom pose estimation models like PoseNet.\n",
    "- **BlazePose:** Built specifically for fast and accurate single-person pose estimation.\n",
    "\n",
    "## 3. What is MediaPipe?\n",
    "MediaPipe is a cross-platform framework by Google that offers efficient pipelines for machine learning and computer vision tasks, such as pose estimation, hand tracking, and facial landmark detection. It supports mobile devices, web, and desktop environments with pre-trained models.\n",
    "\n",
    "\n",
    "## 4. Landmarks in MediaPipe:\n",
    "Landmarks are specific points on the body that MediaPipe detects to represent key parts like joints (e.g., elbows, knees) or regions (e.g., shoulders, hips).\n",
    "- For pose estimation, MediaPipe's **Pose** solution identifies 33 3D landmarks across the human body, enabling applications like fitness tracking, gesture recognition, and AR/VR interactions.\n",
    "![image](https://ai.google.dev/static/edge/mediapipe/images/solutions/hand-landmarks.png)\n",
    "## 5. Brief Explanation:\n",
    "Pose estimation using MediaPipe is highly efficient and lightweight, making it suitable for real-time applications on mobile and desktop platforms. Its built-in models for detecting landmarks are pre-trained, ensuring high accuracy with minimal resource usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 08:28:46.889564: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 08:28:47.704103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes modules from the Mediapipe library, which is used for various machine learning solutions, including hand tracking and gesture recognition. Here's what each line means:\n",
    "```python\n",
    "mp_hands = mp.solutions.hands\n",
    "```\n",
    "`mp.solutions.hands` refers to the Hands solution provided by Mediapipe.\n",
    "This module is designed for hand detection and tracking, including landmark estimation for each finger joint.\n",
    "By assigning it to mp_hands, you create a shorthand to access its functionality in your code.\n",
    "```python\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "```\n",
    "`mp.solutions.drawing_utils` is a utility module for visualizing results.\n",
    "It includes functions to draw detected landmarks and connections (like joints and bones in the hand) on images or video frames.\n",
    "By assigning it to mp_drawing, you can easily use these drawing functions to display the detected hand landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Feature Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(frame, hands):\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process frame with Mediapipe\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Frame dimensions\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Initialize numpy arrays for left and right hands\n",
    "    left_hand_landmarks = np.zeros((21, 2))\n",
    "    right_hand_landmarks = np.zeros((21, 2))\n",
    "\n",
    "    # Helper function to process landmarks\n",
    "    def process_landmarks(hand_landmarks, width, height):\n",
    "        landmarks = [(lm.x * width, lm.y * height) for lm in hand_landmarks.landmark]\n",
    "        landmark_0 = np.array(landmarks[0])\n",
    "        landmark_5 = np.array(landmarks[5])\n",
    "        normalized_landmarks = [\n",
    "            ((x - landmark_0[0]) / (landmark_5[0] - landmark_0[0] + 1e-6),\n",
    "             (y - landmark_0[1]) / (landmark_5[1] - landmark_0[1] + 1e-6))\n",
    "            for x, y in landmarks\n",
    "        ]\n",
    "        return np.array(normalized_landmarks)\n",
    "\n",
    "    # If hands are detected\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            # Identify hand as left or right\n",
    "            handedness = hand_handedness.classification[0].label\n",
    "            processed_landmarks = process_landmarks(hand_landmarks, width, height)\n",
    "            if handedness == 'Left':\n",
    "                left_hand_landmarks = processed_landmarks\n",
    "            elif handedness == 'Right':\n",
    "                right_hand_landmarks = processed_landmarks\n",
    "\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Concatenate left and right hand landmarks\n",
    "    concatenated_landmarks = np.concatenate((left_hand_landmarks.flatten(), right_hand_landmarks.flatten()))\n",
    "    return concatenated_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks Reader and Dataset Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_landmarks(dataset, labels):\n",
    "    # Initialize Mediapipe Hands and Drawing\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=2,\n",
    "                           min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "    kelas = np.eye(len(labels))\n",
    "    y = []\n",
    "    \"\"\"\n",
    "    Reads all `.jpg` images from multiple label directories inside the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The base directory of the dataset.\n",
    "        labels (list): List of label names (subdirectories).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are label names and values are lists of image frames.\n",
    "    \"\"\"\n",
    "    images_by_label = {}\n",
    "    X = []\n",
    "    for i, label in enumerate(labels):\n",
    "        # Construct the directory path\n",
    "        directory_path = os.path.join(dataset, label)\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"Directory '{directory_path}' does not exist.\")\n",
    "            images_by_label[label] = []\n",
    "            continue\n",
    "\n",
    "        # List all `.jpg` files in the directory\n",
    "        filenames = [f for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "\n",
    "        # Read images and store them as frames\n",
    "        # frames = []\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            frame = cv2.imread(file_path)  # Read the image with OpenCV\n",
    "            fitur = feature_extract(frame, hands)\n",
    "            X.append(fitur)\n",
    "            y.append(kelas[i])\n",
    "\n",
    "    hands.close()\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: `read_landmarks`\n",
    "\n",
    "The function `read_landmarks` is designed to:\n",
    "\n",
    "1. Traverse a dataset organized by labels (subdirectories).\n",
    "1. Read all `.jpg` images within each subdirectory.\n",
    "1. Extract features from each image using Mediapipe.\n",
    "1. Prepare the data (`X`) and labels (`y`) for use in machine learning models.\n",
    "\n",
    "### Function Breakdown\n",
    "Parameters\n",
    "\n",
    "1. `dataset` (str): The root directory containing subdirectories for each label.\n",
    "1. `labels` (list): A list of label names, where each corresponds to a subdirectory in the `dataset`.\n",
    "\n",
    "### Step 1: Mediapipe Hands is initialized to process images:\n",
    "\n",
    "1. `static_image_mode=False`: Operates in dynamic mode for video or multiple frames.\n",
    "1. `max_num_hands=2`: Tracks up to 2 hands.\n",
    "1. `min_detection_confidence=0.5`: Minimum confidence to detect a hand.\n",
    "1. `min_tracking_confidence=0.5`: Minimum confidence for hand landmark tracking.\n",
    "\n",
    "### Step 2: One-Hot Encode Labels\n",
    "```python\n",
    "kelas = np.eye(len(labels))\n",
    "y = []\n",
    "```\n",
    "A one-hot encoded matrix (`kelas`) is created for the labels. For example, if there are 3 labels:\n",
    "```lua\n",
    "[[1, 0, 0],\n",
    " [0, 1, 0],\n",
    " [0, 0, 1]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code explanation\n",
    "\n",
    "The function `feature_extract` extracts features from a video frame by processing hand landmarks detected by Mediapipe Hands. It outputs normalized 2D landmark coordinates for both left and right hands.\n",
    "\n",
    "### Parameters\n",
    "1. `frame`: A single video frame (image) from which hand landmarks are to be extracted.\n",
    "1. `hands`: An instance of Mediapipe's `Hands` class for hand detection and landmark tracking.\n",
    "### Function outputs\n",
    "Output\n",
    "\n",
    "The function returns a 1D numpy array containing 84 values:\n",
    "\n",
    "1. 42 values for the left hand (21 points × 2 coordinates: x, y).\n",
    "2. 42 values for the right hand (21 points × 2 coordinates: x, y).\n",
    "\n",
    "If no hand is detected, the corresponding values are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Landmark Extraction using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inisialisasi Mediapipe Hands dan Drawing\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Konfigurasi tangan\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=2,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "# Buka webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Gagal membaca frame dari webcam\")\n",
    "        break\n",
    "\n",
    "    # Konversi ke RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Proses frame\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Jika ada tangan yang terdeteksi\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Gambar landmark tangan\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow('Hand Landmark Detection', frame)\n",
    "\n",
    "    # Berhenti jika tombol 'q' ditekan\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Lepaskan resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
