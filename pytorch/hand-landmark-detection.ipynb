{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pose Estimation Overview\n",
    "\n",
    "## 1. What is Pose Estimation?\n",
    "Pose estimation is a computer vision technique used to identify and track human body parts and their positions in an image or video. It predicts the coordinates of key points (e.g., joints, limbs) to model the pose or movement of the body.\n",
    "\n",
    "## 2. Software for Pose Estimation:\n",
    "Common software and frameworks include:\n",
    "- **MediaPipe:** A lightweight framework by Google for real-time pose detection.\n",
    "- **OpenPose:** An open-source library for multi-person pose estimation.\n",
    "- **TensorFlow.js or PyTorch:** For implementing custom pose estimation models like PoseNet.\n",
    "- **BlazePose:** Built specifically for fast and accurate single-person pose estimation.\n",
    "\n",
    "## 3. What is MediaPipe?\n",
    "MediaPipe is a cross-platform framework by Google that offers efficient pipelines for machine learning and computer vision tasks, such as pose estimation, hand tracking, and facial landmark detection. It supports mobile devices, web, and desktop environments with pre-trained models.\n",
    "\n",
    "\n",
    "## 4. Landmarks in MediaPipe:\n",
    "Landmarks are specific points on the body that MediaPipe detects to represent key parts like joints (e.g., elbows, knees) or regions (e.g., shoulders, hips).\n",
    "- For pose estimation, MediaPipe's **Pose** solution identifies 33 3D landmarks across the human body, enabling applications like fitness tracking, gesture recognition, and AR/VR interactions.\n",
    "![image](https://ai.google.dev/static/edge/mediapipe/images/solutions/hand-landmarks.png)\n",
    "## 5. Brief Explanation:\n",
    "Pose estimation using MediaPipe is highly efficient and lightweight, making it suitable for real-time applications on mobile and desktop platforms. Its built-in models for detecting landmarks are pre-trained, ensuring high accuracy with minimal resource usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 08:28:46.889564: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 08:28:47.704103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code initializes modules from the Mediapipe library, which is used for various machine learning solutions, including hand tracking and gesture recognition. Here's what each line means:\n",
    "```python\n",
    "mp_hands = mp.solutions.hands\n",
    "```\n",
    "`mp.solutions.hands` refers to the Hands solution provided by Mediapipe.\n",
    "This module is designed for hand detection and tracking, including landmark estimation for each finger joint.\n",
    "By assigning it to mp_hands, you create a shorthand to access its functionality in your code.\n",
    "```python\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "```\n",
    "`mp.solutions.drawing_utils` is a utility module for visualizing results.\n",
    "It includes functions to draw detected landmarks and connections (like joints and bones in the hand) on images or video frames.\n",
    "By assigning it to mp_drawing, you can easily use these drawing functions to display the detected hand landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Feature Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(frame, hands):\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process frame with Mediapipe\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Frame dimensions\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Initialize numpy arrays for left and right hands\n",
    "    left_hand_landmarks = np.zeros((21, 2))\n",
    "    right_hand_landmarks = np.zeros((21, 2))\n",
    "\n",
    "    # Helper function to process landmarks\n",
    "    def process_landmarks(hand_landmarks, width, height):\n",
    "        landmarks = [(lm.x * width, lm.y * height) for lm in hand_landmarks.landmark]\n",
    "        landmark_0 = np.array(landmarks[0])\n",
    "        landmark_5 = np.array(landmarks[5])\n",
    "        normalized_landmarks = [\n",
    "            ((x - landmark_0[0]) / (landmark_5[0] - landmark_0[0] + 1e-6),\n",
    "             (y - landmark_0[1]) / (landmark_5[1] - landmark_0[1] + 1e-6))\n",
    "            for x, y in landmarks\n",
    "        ]\n",
    "        return np.array(normalized_landmarks)\n",
    "\n",
    "    # If hands are detected\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_landmarks, hand_handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            # Identify hand as left or right\n",
    "            handedness = hand_handedness.classification[0].label\n",
    "            processed_landmarks = process_landmarks(hand_landmarks, width, height)\n",
    "            if handedness == 'Left':\n",
    "                left_hand_landmarks = processed_landmarks\n",
    "            elif handedness == 'Right':\n",
    "                right_hand_landmarks = processed_landmarks\n",
    "\n",
    "            # Draw landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Concatenate left and right hand landmarks\n",
    "    concatenated_landmarks = np.concatenate((left_hand_landmarks.flatten(), right_hand_landmarks.flatten()))\n",
    "    return concatenated_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code explanation `feature_extract`\n",
    "\n",
    "The function `feature_extract` extracts features from a video frame by processing hand landmarks detected by Mediapipe Hands. It outputs normalized 2D landmark coordinates for both left and right hands.\n",
    "\n",
    "### Parameters\n",
    "1. `frame`: A single video frame (image) from which hand landmarks are to be extracted.\n",
    "1. `hands`: An instance of Mediapipe's `Hands` class for hand detection and landmark tracking.\n",
    "### Function outputs\n",
    "Output\n",
    "\n",
    "The function returns a 1D numpy array containing 84 values:\n",
    "\n",
    "1. 42 values for the left hand (21 points × 2 coordinates: x, y).\n",
    "2. 42 values for the right hand (21 points × 2 coordinates: x, y).\n",
    "\n",
    "If no hand is detected, the corresponding values are zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks Reader and Dataset Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_landmarks(dataset, labels):\n",
    "    # Initialize Mediapipe Hands and Drawing\n",
    "    hands = mp_hands.Hands(static_image_mode=False,\n",
    "                           max_num_hands=2,\n",
    "                           min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5)\n",
    "\n",
    "    kelas = np.eye(len(labels))\n",
    "    y = []\n",
    "    \"\"\"\n",
    "    Reads all `.jpg` images from multiple label directories inside the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): The base directory of the dataset.\n",
    "        labels (list): List of label names (subdirectories).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are label names and values are lists of image frames.\n",
    "    \"\"\"\n",
    "    images_by_label = {}\n",
    "    X = []\n",
    "    for i, label in enumerate(labels):\n",
    "        # Construct the directory path\n",
    "        directory_path = os.path.join(dataset, label)\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"Directory '{directory_path}' does not exist.\")\n",
    "            images_by_label[label] = []\n",
    "            continue\n",
    "\n",
    "        # List all `.jpg` files in the directory\n",
    "        filenames = [f for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "\n",
    "        # Read images and store them as frames\n",
    "        # frames = []\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            frame = cv2.imread(file_path)  # Read the image with OpenCV\n",
    "            fitur = feature_extract(frame, hands)\n",
    "            X.append(fitur)\n",
    "            y.append(kelas[i])\n",
    "\n",
    "    hands.close()\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation: `read_landmarks`\n",
    "\n",
    "The function `read_landmarks` is designed to:\n",
    "\n",
    "1. Traverse a dataset organized by labels (subdirectories).\n",
    "1. Read all `.jpg` images within each subdirectory.\n",
    "1. Extract features from each image using Mediapipe.\n",
    "1. Prepare the data (`X`) and labels (`y`) for use in machine learning models.\n",
    "\n",
    "### Function Breakdown\n",
    "Parameters\n",
    "\n",
    "1. `dataset` (str): The root directory containing subdirectories for each label.\n",
    "1. `labels` (list): A list of label names, where each corresponds to a subdirectory in the `dataset`.\n",
    "\n",
    "### Step 1: Mediapipe Hands is initialized to process images:\n",
    "\n",
    "1. `static_image_mode=False`: Operates in dynamic mode for video or multiple frames.\n",
    "1. `max_num_hands=2`: Tracks up to 2 hands.\n",
    "1. `min_detection_confidence=0.5`: Minimum confidence to detect a hand.\n",
    "1. `min_tracking_confidence=0.5`: Minimum confidence for hand landmark tracking.\n",
    "\n",
    "### Step 2: One-Hot Encode Labels\n",
    "```python\n",
    "kelas = np.eye(len(labels))\n",
    "y = []\n",
    "```\n",
    "A one-hot encoded matrix (`kelas`) is created for the labels. For example, if there are 3 labels:\n",
    "```lua\n",
    "[[1, 0, 0],\n",
    " [0, 1, 0],\n",
    " [0, 0, 1]]\n",
    "```\n",
    "`y` is initialized to store the labels corresponding to each image.\n",
    "\n",
    "### Step 3: Iterate Over Labels\n",
    "```python\n",
    "for i, label in enumerate(labels):\n",
    "    directory_path = os.path.join(dataset, label)\n",
    "```\n",
    "Loops through each label and constructs the full path to its corresponding directory.\n",
    "\n",
    "### Step 4: List All `.jpg` Files\n",
    "```python\n",
    "filenames = [f for f in os.listdir(directory_path) if f.endswith('.jpg')]\n",
    "```\n",
    "Retrieves all filenames ending in `.jpg` from the directory.\n",
    "\n",
    "### Step 5: Read and Process Images\n",
    "```python\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    frame = cv2.imread(file_path)\n",
    "    fitur = ekstraksi_fitur(frame, hands)\n",
    "    X.append(fitur)\n",
    "    y.append(kelas[i])\n",
    "```\n",
    "\n",
    "1. For each image file:\n",
    "    1. Constructs the full file path.\n",
    "    2. Reads the image using OpenCV (`cv2.imread`).\n",
    "    3. Calls `feature_extract` to extract hand landmark features\n",
    "    4. Normalized features for left and right hands are extracted using Mediapipe.\n",
    "    5. Appends the extracted features to `X` and the corresponding one-hot encoded label to `y`.\n",
    "\n",
    "\n",
    "### Output\n",
    "\n",
    "1. `X`: A numpy array of extracted features for all images.\n",
    "1. `y`: A numpy array of one-hot encoded labels corresponding to each feature in `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion from numpy array to Tensor\n",
    "The following function is used to convert from numpy array to Tensor. This conversion is intended for Pytorch dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    return TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Landmark Extraction using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "dataset = \"dataset\"                  # Base dataset directory\n",
    "labels = [\"Satu\", \"Dua\"]     # List of labels (subdirectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732154245.260926   15604 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1732154245.263658   20591 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: llvmpipe (LLVM 15.0.7, 256 bits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 84) (49, 2) (13, 84) (13, 2)\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "X,y = read_landmarks(dataset, labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = X_train.shape[1]  # Number of features (flattened landmarks)\n",
    "num_classes = len(labels)      # Number of labels\n",
    "batch_size = 32                # Batch size for DataLoader\n",
    "learning_rate = 0.001          # Learning rate\n",
    "num_epochs = 20                # Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, loss function, and optimizer\n",
    "model = HandGestureCNN(input_size=input_size, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_dataset = prepare_data(X_train, y_train)\n",
    "test_dataset = prepare_data(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6673\n",
      "Epoch [2/20], Loss: 0.6448\n",
      "Epoch [3/20], Loss: 0.6185\n",
      "Epoch [4/20], Loss: 0.5765\n",
      "Epoch [5/20], Loss: 0.5708\n",
      "Epoch [6/20], Loss: 0.5554\n",
      "Epoch [7/20], Loss: 0.5180\n",
      "Epoch [8/20], Loss: 0.5029\n",
      "Epoch [9/20], Loss: 0.4920\n",
      "Epoch [10/20], Loss: 0.4533\n",
      "Epoch [11/20], Loss: 0.4492\n",
      "Epoch [12/20], Loss: 0.4226\n",
      "Epoch [13/20], Loss: 0.3885\n",
      "Epoch [14/20], Loss: 0.3697\n",
      "Epoch [15/20], Loss: 0.3475\n",
      "Epoch [16/20], Loss: 0.3342\n",
      "Epoch [17/20], Loss: 0.3179\n",
      "Epoch [18/20], Loss: 0.3047\n",
      "Epoch [19/20], Loss: 0.2644\n",
      "Epoch [20/20], Loss: 0.2471\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))  # One-hot to index\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.31%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving for further purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"outputs\"\n",
    "model_name = \"model.pt\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)  # Buat direktori jika belum ada\n",
    "    print(f\"Directory '{output_path}' created.\")\n",
    "torch.save(model.state_dict(), os.path.join(output_path,model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing\n",
    "\n",
    "Let's load our model from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join(output_path,model_name)\n",
    "model = HandGestureCNN(input_size=input_size, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.31%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
